# RAG (Retrieval-Augmented Generation) for Knowledge Base
# Answers questions using video transcripts

import logging
from typing import Optional

from openai import AsyncOpenAI

from config.settings import OPENAI_API_KEY
from knowledge_base.db_manager import search_chunks, get_knowledge_stats

logger = logging.getLogger(__name__)


class KnowledgeRAG:
    """RAG system for answering questions from video knowledge base."""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
        self.embedding_model = "text-embedding-3-small"
        self.chat_model = "gpt-4o-mini"
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        self.brief_prompt = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ñ—Ä–∞–Ω—á–∞–π–∑–∏ –±–∞—Ä–±–µ—Ä—à–æ–ø–æ–≤ BORODACH.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –ö–†–ê–¢–ö–ò–ô –æ—Ç–≤–µ—Ç (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
2. –£–∫–∞–∂–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —É—Ä–æ–∫–∞ –≤ –∫–æ–Ω—Ü–µ: "üìπ –£—Ä–æ–∫: {–Ω–∞–∑–≤–∞–Ω–∏–µ}"
3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º

–ù–ï –ü–ò–®–ò –¥–ª–∏–Ω–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è ‚Äî —Ç–æ–ª—å–∫–æ —Å—É—Ç—å!
"""
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        self.detailed_prompt = """–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ñ—Ä–∞–Ω—á–∞–π–∑–∏ –±–∞—Ä–±–µ—Ä—à–æ–ø–æ–≤ BORODACH.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –ü–û–î–†–û–ë–ù–´–ô –∏ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û–±—ä—è—Å–Ω–∏ —Ç–µ–º—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω–æ
2. –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —Ç–µ–∑–∏—Å—ã, –ø—Ä–∏–º–µ—Ä—ã, —à–∞–≥–∏
3. –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—Å–ø–µ–∫—Ç–æ–≤ ‚Äî —Ä–∞—Å–∫—Ä–æ–π –∫–∞–∂–¥—ã–π
4. –í –∫–æ–Ω—Ü–µ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: "üìπ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {–Ω–∞–∑–≤–∞–Ω–∏—è —É—Ä–æ–∫–æ–≤}"

–û—Ç–≤–µ—á–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ, –∫–∞–∫ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –Ω–∞ –ª–µ–∫—Ü–∏–∏!
"""
        
        # –°—Ç–∞—Ä—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.system_prompt = self.brief_prompt
    
    async def create_query_embedding(self, query: str) -> Optional[list[float]]:
        """Create embedding for search query."""
        if not self.client:
            logger.error("[RAG] OpenAI client not configured")
            return None
        
        try:
            response = await self.client.embeddings.create(
                model=self.embedding_model,
                input=query
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"[RAG] Error creating embedding: {e}")
            return None
    
    async def search(self, query: str, limit: int = 3) -> list[dict]:
        """Search for relevant chunks in knowledge base."""
        embedding = await self.create_query_embedding(query)
        if not embedding:
            return []
        
        results = await search_chunks(embedding, limit=limit)
        logger.info(f"[RAG] Found {len(results)} relevant chunks for query: {query[:50]}...")
        return results
    
    def format_context(self, chunks: list[dict]) -> str:
        """Format search results as context for GPT."""
        if not chunks:
            return "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω."
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}]\n"
                f"–£—Ä–æ–∫: {chunk.get('lesson_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
                f"–¢–µ–∫—Å—Ç: {chunk.get('text', '')}\n"
            )
        
        return "\n---\n".join(context_parts)
    
    async def answer_question_brief(self, question: str) -> Optional[dict]:
        """
        Answer a question briefly, returning answer and context for follow-up.
        
        Returns:
            dict with 'answer' and 'context' keys, or None if failed
        """
        if not self.client:
            return None
        
        # Check if knowledge base has data
        stats = await get_knowledge_stats()
        if stats["embedded_count"] == 0:
            return None
        
        # Search for relevant chunks
        chunks = await self.search(question, limit=3)
        
        if not chunks:
            return None
        
        # Format context
        context = self.format_context(chunks)
        
        try:
            logger.info(f"[RAG] Generating BRIEF answer for: {question[:50]}...")
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": self.brief_prompt},
                    {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ:\n\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
                ],
                temperature=0.3,
                max_tokens=200,  # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                timeout=15.0
            )
            
            answer = response.choices[0].message.content
            logger.info(f"[RAG] Brief answer generated (tokens: {response.usage.total_tokens})")
            
            return {
                "answer": answer,
                "context": context
            }
            
        except Exception as e:
            logger.error(f"[RAG] Error generating brief answer: {e}")
            return None
    
    async def answer_question_detailed(self, question: str, context: str) -> Optional[str]:
        """
        Answer a question in detail using pre-saved context.
        
        Args:
            question: Original question
            context: Pre-saved context from brief answer
            
        Returns:
            Detailed answer string or None if failed
        """
        if not self.client:
            return None
        
        try:
            logger.info(f"[RAG] Generating DETAILED answer for: {question[:50]}...")
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": self.detailed_prompt},
                    {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–∏–¥–µ–æ:\n\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
                ],
                temperature=0.4,
                max_tokens=1000,  # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç
                timeout=30.0
            )
            
            answer = response.choices[0].message.content
            logger.info(f"[RAG] Detailed answer generated (tokens: {response.usage.total_tokens})")
            
            return answer
            
        except Exception as e:
            logger.error(f"[RAG] Error generating detailed answer: {e}")
            return None
    
    async def answer(self, question: str) -> str:
        """
        Answer a question using RAG (legacy method for compatibility).
        Returns formatted answer with video references.
        """
        result = await self.answer_question_brief(question)
        if result:
            return result["answer"]
        return (
            "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.\n\n"
            "üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –æ—Ñ–∏—Å "
            "—á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª ¬´üìö –ü–æ–ª–µ–∑–Ω–æ–µ¬ª."
        )
    
    async def is_knowledge_question(self, text: str) -> bool:
        """
        Check if the text is a question that should be answered from knowledge base.
        Returns True if it looks like a question about franchise operations.
        """
        # Keywords that suggest a knowledge question
        knowledge_keywords = [
            "–∫–∞–∫", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∫–æ–≥–¥–∞", "–≥–¥–µ", "—á—Ç–æ —Ç–∞–∫–æ–µ",
            "—Å–∫–æ–ª—å–∫–æ", "–∫–∞–∫–æ–π", "–∫–∞–∫–∞—è", "–∫–∞–∫–∏–µ",
            "—Ä–∞—Å—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ–¥—Å–∫–∞–∂–∏", "–ø–æ–º–æ–≥–∏",
            "–¥–µ–ª–∞—Ç—å", "—Ä–∞–±–æ—Ç–∞—Ç—å", "–æ—Ñ–æ—Ä–º–∏—Ç—å", "–ø–æ–ª—É—á–∏—Ç—å",
            "–∫–ª–∏–µ–Ω—Ç", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫", "–∫–∞—Å—Å–∞", "–≤—ã—Ä—É—á–∫–∞", "–∑–∞—Ä–ø–ª–∞—Ç–∞",
            "–æ–±—É—á–µ–Ω–∏–µ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç",
            "yclients", "–±–∏—Ç—Ä–∏–∫—Å", "bitrix",
        ]
        
        text_lower = text.lower()
        
        # Check for question marks or keywords
        if "?" in text:
            return True
        
        for keyword in knowledge_keywords:
            if keyword in text_lower:
                return True
        
        return False


# Singleton instance
knowledge_rag = KnowledgeRAG()

